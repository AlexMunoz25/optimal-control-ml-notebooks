{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37bf6c98",
   "metadata": {},
   "source": [
    "### A3.2.1. Single Instruction Multiple Data Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a658cb",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Speedup}_{\\text{SIMD}} = \\frac{n}{\\lceil n / w \\rceil}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of elements and $w$ is the SIMD lane width (elements per vector register)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c98cce",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**SIMD (Single Instruction, Multiple Data)** executes one instruction on multiple data elements simultaneously using wide vector registers. Instead of adding two scalars, a single SIMD add processes 4, 8, or 16 elements in one cycle.\n",
    "\n",
    "**x86 SIMD Instruction Sets:**\n",
    "\n",
    "| ISA Extension | Register Width | Float Lanes (32-bit) | Year |\n",
    "|---------------|---------------|---------------------|------|\n",
    "| SSE           | 128-bit       | 4                    | 1999 |\n",
    "| AVX           | 256-bit       | 8                    | 2011 |\n",
    "| AVX-512       | 512-bit       | 16                   | 2017 |\n",
    "\n",
    "**ARM SIMD:**\n",
    "\n",
    "| ISA Extension | Register Width | Float Lanes (32-bit) |\n",
    "|---------------|---------------|---------------------|\n",
    "| NEON          | 128-bit       | 4                    |\n",
    "| SVE/SVE2      | 128‚Äì2048-bit  | Variable             |\n",
    "\n",
    "**Requirements for SIMD:**\n",
    "\n",
    "- **Data parallelism** ‚Äî the same operation on independent elements.\n",
    "- **Contiguous memory layout** ‚Äî aligned, stride-1 access patterns are fastest.\n",
    "- **No cross-lane dependencies** ‚Äî each lane operates independently (reductions need special shuffle/horizontal ops).\n",
    "\n",
    "**Sources of SIMD Code:**\n",
    "\n",
    "1. **Compiler auto-vectorization** ‚Äî the compiler detects vectorizable loops.\n",
    "2. **Intrinsics** ‚Äî explicit SIMD calls (`_mm256_add_ps`).\n",
    "3. **Libraries** ‚Äî NumPy, Eigen, etc. use SIMD internally.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Adding two arrays of 8 floats:\n",
    "- Scalar: 8 `fadd` instructions.\n",
    "- AVX: 1 `vaddps ymm` instruction (8 floats in one 256-bit register)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a63c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "SIZE = 10_000_000\n",
    "array_a = np.random.rand(SIZE).astype(np.float32)\n",
    "array_b = np.random.rand(SIZE).astype(np.float32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "scalar_result = np.empty(SIZE, dtype=np.float32)\n",
    "for index in range(SIZE):\n",
    "    scalar_result[index] = array_a[index] + array_b[index]\n",
    "scalar_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "vector_result = array_a + array_b\n",
    "vector_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Elements: {SIZE:,}\")\n",
    "print(f\"Scalar loop: {scalar_time:.3f}s\")\n",
    "print(f\"NumPy vectorized: {vector_time:.6f}s\")\n",
    "print(f\"Speedup: {scalar_time / vector_time:.0f}x\")\n",
    "print(f\"Results match: {np.allclose(scalar_result, vector_result)}\")\n",
    "\n",
    "simd_widths = {\n",
    "    \"SSE (128-bit)\": 128,\n",
    "    \"AVX (256-bit)\": 256,\n",
    "    \"AVX-512 (512-bit)\": 512,\n",
    "}\n",
    "element_bits = 32\n",
    "\n",
    "print(f\"\\nTheoretical lanes for {element_bits}-bit floats:\")\n",
    "for name, width in simd_widths.items():\n",
    "    lanes = width // element_bits\n",
    "    print(f\"  {name}: {lanes} lanes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd89d07c",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò Intel Corporation. *Intel Intrinsics Guide.*](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)\n",
    "\n",
    "[üìò Hennessy, J. & Patterson, D. (2019). *Computer Architecture: A Quantitative Approach (6th ed.).* Morgan Kaufmann.](https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: Branch Prediction](../01_Central_Processing_Unit_Performance/03_branch_prediction.ipynb) | [Next: Vectorization Reports ‚û°Ô∏è](./02_vectorization_reports.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
