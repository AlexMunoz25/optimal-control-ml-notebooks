{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3abbe737",
   "metadata": {},
   "source": [
    "### A4.1.2. TensorFlow XLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01598d",
   "metadata": {},
   "source": [
    "> *XLA (Accelerated Linear Algebra) is TensorFlow's domain-specific compiler that fuses operations and generates optimized code for CPUs, GPUs, and TPUs from TensorFlow graphs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136a065",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "In TensorFlow, XLA compiles subgraphs of the computation graph into fused kernels. Without XLA, each TF op dispatches a separate precompiled kernel; with XLA, multiple ops are fused into one kernel, eliminating intermediate memory traffic.\n",
    "\n",
    "**How TF Invokes XLA:**\n",
    "\n",
    "| Mechanism | Description |\n",
    "|-----------|------------|\n",
    "| `tf.function(jit_compile=True)` | Explicitly compile the traced function with XLA |\n",
    "| Auto-clustering | TF runtime identifies fusible subgraphs and compiles them |\n",
    "| `TF_XLA_FLAGS=--tf_xla_auto_jit=2` | Environment variable to enable aggressive auto-clustering |\n",
    "\n",
    "**XLA Compilation Flow in TF:**\n",
    "\n",
    "```\n",
    "tf.function ‚Üí TF Graph ‚Üí XLA HLO ‚Üí XLA optimizations ‚Üí LLVM IR ‚Üí machine code\n",
    "```\n",
    "\n",
    "**Key Optimizations:**\n",
    "\n",
    "- **Op fusion** ‚Äî elementwise chains become one kernel (e.g., `relu(matmul(x, w) + b)`).\n",
    "- **Buffer assignment** ‚Äî aliases output buffers to inputs when safe, avoiding copies.\n",
    "- **Layout optimization** ‚Äî transposes data into hardware-preferred layout (e.g., NHWC ‚Üí NCHW for GPU).\n",
    "- **Constant folding** ‚Äî evaluates ops with known inputs at compile time.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Dynamic shapes require recompilation for each distinct shape.\n",
    "- Not all TF ops have XLA lowerings (e.g., `tf.py_function`).\n",
    "- Compilation adds latency on first call.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "@tf.function(jit_compile=True)\n",
    "def fused_layer(x, w, b):\n",
    "    return tf.nn.relu(tf.matmul(x, w) + b)\n",
    "```\n",
    "\n",
    "Without XLA: 3 kernel launches (matmul, add, relu). With XLA: 1 fused kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TFOp:\n",
    "    name: str\n",
    "    inputs: list[str]\n",
    "    output: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class XLACluster:\n",
    "    ops: list[TFOp] = field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def kernel_count_unfused(self):\n",
    "        return len(self.ops)\n",
    "\n",
    "    @property\n",
    "    def kernel_count_fused(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def memory_roundtrips_saved(self):\n",
    "        return len(self.ops) - 1\n",
    "\n",
    "\n",
    "graph_ops = [\n",
    "    TFOp(\"MatMul\", [\"%x\", \"%w\"], \"%mm\"),\n",
    "    TFOp(\"BiasAdd\", [\"%mm\", \"%b\"], \"%add\"),\n",
    "    TFOp(\"Relu\", [\"%add\"], \"%out\"),\n",
    "]\n",
    "\n",
    "cluster = XLACluster(ops=graph_ops)\n",
    "\n",
    "print(\"TF Graph (unfused):\")\n",
    "for op in graph_ops:\n",
    "    print(f\"  {op.output} = {op.name}({', '.join(op.inputs)})\")\n",
    "\n",
    "print(f\"\\nWithout XLA: {cluster.kernel_count_unfused} kernel launches\")\n",
    "print(f\"With XLA:    {cluster.kernel_count_fused} fused kernel\")\n",
    "print(f\"Memory roundtrips eliminated: {cluster.memory_roundtrips_saved}\")\n",
    "\n",
    "compilation_mechanisms = [\n",
    "    (\"tf.function(jit_compile=True)\", \"explicit, per-function\"),\n",
    "    (\"auto-clustering\", \"runtime identifies fusible subgraphs\"),\n",
    "    (\"TF_XLA_FLAGS=--tf_xla_auto_jit=2\", \"aggressive global auto-clustering\"),\n",
    "]\n",
    "\n",
    "print(\"\\nXLA activation methods:\")\n",
    "for method, description in compilation_mechanisms:\n",
    "    print(f\"  {method}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74176432",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò TensorFlow. *XLA: Optimizing Compiler for Machine Learning.*](https://www.tensorflow.org/xla)\n",
    "\n",
    "[üìò OpenXLA Project. *XLA Architecture.*](https://openxla.org/xla/architecture)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: OpenXLA](./01_openxla.ipynb) | [Next: JAX Compilation ‚û°Ô∏è](./03_jax_compilation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
