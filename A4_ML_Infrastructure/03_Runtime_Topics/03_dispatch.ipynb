{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70214b99",
   "metadata": {},
   "source": [
    "### A4.3.3. Dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120a5190",
   "metadata": {},
   "source": [
    "> *Dispatch is the runtime mechanism that selects and launches the correct kernel implementation for an operation based on the device, data type, layout, and other properties of its inputs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da5933",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "When a framework executes an operation like `matmul`, the **dispatcher** must resolve which concrete kernel to invoke. This decision depends on multiple axes:\n",
    "\n",
    "**Dispatch Axes:**\n",
    "\n",
    "| Axis | Examples |\n",
    "|------|----------|\n",
    "| Device | CPU, CUDA, TPU, XPU |\n",
    "| Dtype | float32, float16, bfloat16, int8 |\n",
    "| Layout | dense, sparse_coo, sparse_csr, strided |\n",
    "| Autograd | needs_grad ‚Üí wrap with gradient tracking |\n",
    "| Quantization | quantized weight ‚Üí specialized kernel |\n",
    "\n",
    "**PyTorch Dispatcher:**\n",
    "\n",
    "PyTorch uses a **dispatch table** indexed by **dispatch keys** (device, autograd, quantized, etc.). Each op is registered with multiple implementations:\n",
    "\n",
    "```\n",
    "aten::matmul\n",
    "  ‚îú‚îÄ‚îÄ CPU ‚Üí mkl_matmul / openblas_matmul\n",
    "  ‚îú‚îÄ‚îÄ CUDA ‚Üí cublas_matmul / cutlass_matmul\n",
    "  ‚îú‚îÄ‚îÄ AutogradCPU ‚Üí autograd wrapper ‚Üí CPU kernel\n",
    "  ‚îî‚îÄ‚îÄ QuantizedCPU ‚Üí quantized_matmul_int8\n",
    "```\n",
    "\n",
    "**XLA/JAX Dispatch:**\n",
    "\n",
    "In JIT-compiled systems (XLA, JAX), dispatch happens at **compile time**: the compiler selects the kernel during lowering/code generation, not at runtime.\n",
    "\n",
    "**Tradeoffs:**\n",
    "\n",
    "- **Eager dispatch** (PyTorch) ‚Äî flexible, supports dynamic shapes and control flow, per-op overhead (~Œºs).\n",
    "- **Compiled dispatch** (XLA) ‚Äî zero per-op overhead at runtime, but compilation cost upfront.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Calling `torch.matmul(a, b)` where `a` is on CUDA and has `requires_grad=True`:\n",
    "1. Dispatcher checks keys: `AutogradCUDA` ‚Üí `CUDA`.\n",
    "2. Autograd wrapper records the op on the tape.\n",
    "3. CUDA backend dispatches to cuBLAS GEMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DispatchKey:\n",
    "    device: str\n",
    "    dtype: str\n",
    "    requires_grad: bool = False\n",
    "\n",
    "    @property\n",
    "    def key_tuple(self):\n",
    "        return (self.device, self.dtype, self.requires_grad)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class KernelEntry:\n",
    "    dispatch_key: tuple\n",
    "    kernel_name: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DispatchTable:\n",
    "    op_name: str\n",
    "    entries: dict = field(default_factory=dict)\n",
    "\n",
    "    def register(self, device, dtype, requires_grad, kernel_name):\n",
    "        key = (device, dtype, requires_grad)\n",
    "        self.entries[key] = kernel_name\n",
    "\n",
    "    def dispatch(self, dispatch_key):\n",
    "        key = dispatch_key.key_tuple\n",
    "        if key in self.entries:\n",
    "            return self.entries[key]\n",
    "        fallback_key = (dispatch_key.device, dispatch_key.dtype, False)\n",
    "        return self.entries[fallback_key]\n",
    "\n",
    "\n",
    "matmul_table = DispatchTable(\"aten::matmul\")\n",
    "matmul_table.register(\"cpu\", \"float32\", False, \"mkl_sgemm\")\n",
    "matmul_table.register(\"cpu\", \"float64\", False, \"mkl_dgemm\")\n",
    "matmul_table.register(\"cuda\", \"float32\", False, \"cublas_sgemm\")\n",
    "matmul_table.register(\"cuda\", \"float16\", False, \"cublas_hgemm\")\n",
    "matmul_table.register(\"cuda\", \"float32\", True, \"autograd_cuda_sgemm\")\n",
    "matmul_table.register(\"cpu\", \"int8\", False, \"quantized_matmul_int8\")\n",
    "\n",
    "print(f\"Dispatch table: {matmul_table.op_name}\")\n",
    "print(f\"Registered kernels: {len(matmul_table.entries)}\")\n",
    "for key, kernel in matmul_table.entries.items():\n",
    "    device, dtype, grad = key\n",
    "    grad_str = \", grad\" if grad else \"\"\n",
    "    print(f\"  ({device}, {dtype}{grad_str}) ‚Üí {kernel}\")\n",
    "\n",
    "test_cases = [\n",
    "    DispatchKey(\"cuda\", \"float32\", requires_grad=True),\n",
    "    DispatchKey(\"cuda\", \"float16\", requires_grad=False),\n",
    "    DispatchKey(\"cpu\", \"float32\", requires_grad=False),\n",
    "    DispatchKey(\"cpu\", \"int8\", requires_grad=False),\n",
    "]\n",
    "\n",
    "print(\"\\nDispatch resolution:\")\n",
    "for key in test_cases:\n",
    "    kernel = matmul_table.dispatch(key)\n",
    "    grad_str = \", grad\" if key.requires_grad else \"\"\n",
    "    print(f\"  matmul({key.device}, {key.dtype}{grad_str}) ‚Üí {kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ecc1d",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò Chanan, G. *PyTorch Dispatcher Internals.*](http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/)\n",
    "\n",
    "[üìò PyTorch Documentation. *Extending dispatcher for a new backend.*](https://pytorch.org/tutorials/advanced/extend_dispatcher.html)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: Memory Planning](./02_memory_planning.ipynb) | [Next: Benchmark Design ‚û°Ô∏è](../04_Benchmarking_and_Regressions/01_benchmark_design.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
