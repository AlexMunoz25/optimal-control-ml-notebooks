{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e2d26d0",
   "metadata": {},
   "source": [
    "### A4.3.1. Operator Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d8f533",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Bytes}_{\\text{saved}} = \\sum_{i=1}^{n-1} \\text{size}(\\text{intermediate}_i) \\times 2\n",
    "$$\n",
    "\n",
    "where $n$ is the number of fused ops and the factor 2 accounts for one write and one read of each eliminated intermediate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577f5f58",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Operator fusion** merges multiple operations into a single kernel so that intermediate results stay in registers or shared memory instead of being written to and read from global memory (DRAM/HBM).\n",
    "\n",
    "**Why Fusion Matters:**\n",
    "\n",
    "ML workloads are often **memory-bandwidth bound** for elementwise and reduction ops. A chain like `relu(matmul(x, w) + b)` without fusion requires:\n",
    "- Write `matmul` result to memory ‚Üí read it for `add` ‚Üí write `add` result ‚Üí read it for `relu`.\n",
    "\n",
    "With fusion: one kernel computes all three, intermediate values never leave registers.\n",
    "\n",
    "**XLA Fusion Categories:**\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|--------|\n",
    "| Element-wise | All ops are pointwise on same shape | `add ‚Üí relu ‚Üí mul` |\n",
    "| Input fusion | Consumer reads producer's output element-by-element | `broadcast ‚Üí add` |\n",
    "| Output fusion | Producer's output is consumed by a reduction | `matmul ‚Üí bias_add` |\n",
    "| Loop fusion | Ops share a common iteration space | `transpose ‚Üí elementwise` |\n",
    "\n",
    "**Fusion Decisions:**\n",
    "\n",
    "The compiler uses a **cost model** considering:\n",
    "- Register pressure ‚Äî too many fused ops may spill to memory.\n",
    "- Shared memory limits ‚Äî fused kernel must fit tile data.\n",
    "- Recomputation vs. materialization ‚Äî sometimes recomputing is cheaper than storing.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "Unfused: matmul(128√ó784, 784√ó256) ‚Üí [128√ó256 write] ‚Üí add(bias) ‚Üí [128√ó256 write] ‚Üí relu ‚Üí [128√ó256 write]\n",
    "Fused:   matmul+add+relu ‚Üí [128√ó256 write]   (2 intermediate writes eliminated)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b7b74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Operator:\n",
    "    name: str\n",
    "    output_shape: tuple[int, ...]\n",
    "    element_bytes: int = 4\n",
    "\n",
    "    @property\n",
    "    def output_bytes(self):\n",
    "        total_elements = 1\n",
    "        for dim in self.output_shape:\n",
    "            total_elements *= dim\n",
    "        return total_elements * self.element_bytes\n",
    "\n",
    "\n",
    "def analyze_fusion(ops):\n",
    "    unfused_memory_traffic = sum(\n",
    "        op.output_bytes * 2\n",
    "        for op in ops\n",
    "    )\n",
    "    fused_memory_traffic = ops[-1].output_bytes\n",
    "\n",
    "    intermediate_bytes_saved = sum(\n",
    "        op.output_bytes * 2\n",
    "        for op in ops[:-1]\n",
    "    )\n",
    "\n",
    "    return unfused_memory_traffic, fused_memory_traffic, intermediate_bytes_saved\n",
    "\n",
    "\n",
    "ops = [\n",
    "    Operator(\"matmul\", (128, 256)),\n",
    "    Operator(\"bias_add\", (128, 256)),\n",
    "    Operator(\"relu\", (128, 256)),\n",
    "]\n",
    "\n",
    "unfused_traffic, fused_traffic, saved = analyze_fusion(ops)\n",
    "\n",
    "print(\"Operator chain: \" + \" ‚Üí \".join(op.name for op in ops))\n",
    "print(f\"Output shape: {ops[-1].output_shape}\")\n",
    "print(f\"\\nUnfused memory traffic: {unfused_traffic:,} bytes ({unfused_traffic / 1024:.1f} KB)\")\n",
    "print(f\"Fused memory traffic:   {fused_traffic:,} bytes ({fused_traffic / 1024:.1f} KB)\")\n",
    "print(f\"Bytes saved by fusion:  {saved:,} bytes ({saved / 1024:.1f} KB)\")\n",
    "print(f\"Traffic reduction:      {saved / unfused_traffic:.0%}\")\n",
    "\n",
    "larger_ops = [\n",
    "    Operator(\"matmul\", (1024, 1024)),\n",
    "    Operator(\"bias_add\", (1024, 1024)),\n",
    "    Operator(\"relu\", (1024, 1024)),\n",
    "    Operator(\"dropout\", (1024, 1024)),\n",
    "]\n",
    "\n",
    "unfused_large, fused_large, saved_large = analyze_fusion(larger_ops)\n",
    "print(f\"\\nLarger example ({len(larger_ops)} ops, 1024√ó1024):\")\n",
    "print(f\"  Unfused: {unfused_large / (1024**2):.1f} MB\")\n",
    "print(f\"  Fused:   {fused_large / (1024**2):.1f} MB\")\n",
    "print(f\"  Saved:   {saved_large / (1024**2):.1f} MB ({saved_large / unfused_large:.0%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f86eef0",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò OpenXLA Project. *XLA Architecture ‚Äî Fusion.*](https://openxla.org/xla/architecture)\n",
    "\n",
    "[üìò Chen, T. et al. (2018). *TVM: An Automated End-to-End Optimizing Compiler for Deep Learning.* OSDI.](https://www.usenix.org/conference/osdi18/presentation/chen)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: Custom Calls](../02_Framework_Integration/03_custom_calls.ipynb) | [Next: Memory Planning ‚û°Ô∏è](./02_memory_planning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
