{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44811a4b",
   "metadata": {},
   "source": [
    "### A3.3.2. Lock Contention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6bd946",
   "metadata": {},
   "source": [
    "$$\n",
    "T_{\\text{contended}} = T_{\\text{work}} + n \\cdot T_{\\text{lock\\_wait}}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of threads contending for the same lock and $T_{\\text{lock\\_wait}}$ is the average wait time per acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805e9b6",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Lock contention** occurs when multiple threads compete to acquire the same lock, serializing what should be parallel work. High contention degrades throughput and can negate the benefit of additional cores.\n",
    "\n",
    "**Contention Taxonomy:**\n",
    "\n",
    "| Type | Description | Impact |\n",
    "|------|-------------|--------|\n",
    "| Uncontended | Single thread holds lock, no waiters | Low overhead (~20 ns) |\n",
    "| Low contention | Occasional short waits | Acceptable |\n",
    "| High contention | Many threads spin/block on one lock | Near-serial execution |\n",
    "| Convoy | All threads synchronize at lock release | Total serialization |\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "\n",
    "1. **Reduce critical section** ‚Äî hold the lock for the minimum time necessary.\n",
    "2. **Lock striping** ‚Äî split one lock into many, each protecting a subset of data (e.g., `ConcurrentHashMap` uses per-bucket locks).\n",
    "3. **Lock-free data structures** ‚Äî use atomic compare-and-swap (CAS) instead of locks.\n",
    "4. **Read-write locks** ‚Äî allow concurrent readers, exclusive writers.\n",
    "5. **Thread-local accumulation** ‚Äî each thread writes to local storage, merge results after.\n",
    "\n",
    "**The Universal Scalability Law (USL):**\n",
    "\n",
    "$$C(n) = \\frac{n}{1 + \\sigma(n-1) + \\kappa \\, n(n-1)}$$\n",
    "\n",
    "where $\\sigma$ is the contention penalty and $\\kappa$ is the coherence (crosstalk) penalty. As $n$ grows, throughput can actually *decrease* if $\\kappa > 0$.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "A shared counter incremented by 8 threads under a global lock runs slower than a single thread (each increment acquires/releases the lock). With thread-local counters merged at the end, contention drops to zero during the work phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7881c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "\n",
    "INCREMENTS_PER_THREAD = 500_000\n",
    "\n",
    "\n",
    "def contended_increment(shared_counter, lock, count):\n",
    "    for _ in range(count):\n",
    "        with lock:\n",
    "            shared_counter[0] += 1\n",
    "\n",
    "\n",
    "def local_increment(local_results, thread_index, count):\n",
    "    local_sum = 0\n",
    "    for _ in range(count):\n",
    "        local_sum += 1\n",
    "    local_results[thread_index] = local_sum\n",
    "\n",
    "\n",
    "thread_counts = [1, 2, 4, 8]\n",
    "\n",
    "print(\"Contended (global lock):\")\n",
    "for num_threads in thread_counts:\n",
    "    shared_counter = [0]\n",
    "    lock = threading.Lock()\n",
    "    threads = [\n",
    "        threading.Thread(target=contended_increment, args=(shared_counter, lock, INCREMENTS_PER_THREAD))\n",
    "        for _ in range(num_threads)\n",
    "    ]\n",
    "    start = time.perf_counter()\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    total_ops = num_threads * INCREMENTS_PER_THREAD\n",
    "    print(f\"  {num_threads} threads: {elapsed:.3f}s, count={shared_counter[0]:,}, ops/s={total_ops/elapsed:,.0f}\")\n",
    "\n",
    "print(\"\\nUncontended (thread-local):\")\n",
    "for num_threads in thread_counts:\n",
    "    local_results = [0] * num_threads\n",
    "    threads = [\n",
    "        threading.Thread(target=local_increment, args=(local_results, index, INCREMENTS_PER_THREAD))\n",
    "        for index in range(num_threads)\n",
    "    ]\n",
    "    start = time.perf_counter()\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    total_count = sum(local_results)\n",
    "    total_ops = num_threads * INCREMENTS_PER_THREAD\n",
    "    print(f\"  {num_threads} threads: {elapsed:.3f}s, count={total_count:,}, ops/s={total_ops/elapsed:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323bce3",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò Gunther, N. (2007). *Guerrilla Capacity Planning.* Springer.](https://link.springer.com/book/10.1007/978-3-540-31010-5)\n",
    "\n",
    "[üìò Herlihy, M. & Shavit, N. (2012). *The Art of Multiprocessor Programming (Revised 1st ed.).* Morgan Kaufmann.](https://www.elsevier.com/books/the-art-of-multiprocessor-programming/herlihy/978-0-12-397337-5)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: Work Partitioning](./01_work_partitioning.ipynb) | [Next: Linux perf Tool ‚û°Ô∏è](../04_Profiling/01_linux_perf_tool.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
