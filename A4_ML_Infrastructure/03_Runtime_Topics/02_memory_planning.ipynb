{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8dc2bc",
   "metadata": {},
   "source": [
    "### A4.3.2. Memory Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876729ef",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Peak Memory} = \\max_{t} \\sum_{b \\in \\text{live}(t)} \\text{size}(b)\n",
    "$$\n",
    "\n",
    "where $\\text{live}(t)$ is the set of buffers alive at time step $t$ in the execution schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a95526",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "**Memory planning** (buffer assignment) determines when and where to allocate/deallocate tensor buffers during compiled model execution. The goal is to minimize peak memory usage while respecting data dependencies.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Liveness analysis** â€” a buffer is live from its definition until its last use. Non-overlapping lifetimes can share the same memory.\n",
    "- **Buffer aliasing** â€” reuse the input buffer for the output when the op is in-place safe (e.g., elementwise ops).\n",
    "- **Memory fragmentation** â€” even if total free memory suffices, fragmented free regions may prevent allocation.\n",
    "\n",
    "**XLA Buffer Assignment:**\n",
    "\n",
    "1. Schedule HLO instructions (topological order with heuristics).\n",
    "2. Compute live ranges for every buffer.\n",
    "3. Assign non-overlapping buffers to the same memory offset (coloring problem).\n",
    "4. Handle constraints: pinned buffers (inputs/outputs), alignment, aliasing hints.\n",
    "\n",
    "**Strategies:**\n",
    "\n",
    "| Strategy | Description |\n",
    "|----------|------------|\n",
    "| Greedy by size | Assign largest buffers first, reuse biggest available gap |\n",
    "| Offset assignment | Pack buffers at offsets in a contiguous arena |\n",
    "| Rematerialization | Recompute a value instead of storing it (trade compute for memory) |\n",
    "| Gradient checkpointing | Store only selected activations during forward; recompute the rest during backward |\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Three buffers A, B, C where A is live during steps 0â€“2, B during 1â€“3, C during 3â€“5. Buffer A and C have non-overlapping lifetimes â†’ they share the same memory slot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa49665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Buffer:\n",
    "    name: str\n",
    "    size_bytes: int\n",
    "    live_start: int\n",
    "    live_end: int\n",
    "\n",
    "\n",
    "def compute_peak_memory(buffers, total_steps):\n",
    "    peak = 0\n",
    "    peak_step = 0\n",
    "    memory_timeline = []\n",
    "    for step in range(total_steps):\n",
    "        live_buffers = [\n",
    "            buf for buf in buffers\n",
    "            if buf.live_start <= step <= buf.live_end\n",
    "        ]\n",
    "        step_memory = sum(buf.size_bytes for buf in live_buffers)\n",
    "        memory_timeline.append((step, step_memory, [buf.name for buf in live_buffers]))\n",
    "        if step_memory > peak:\n",
    "            peak = step_memory\n",
    "            peak_step = step\n",
    "    return peak, peak_step, memory_timeline\n",
    "\n",
    "\n",
    "def find_sharing_opportunities(buffers):\n",
    "    opportunities = []\n",
    "    for index_a in range(len(buffers)):\n",
    "        for index_b in range(index_a + 1, len(buffers)):\n",
    "            buf_a = buffers[index_a]\n",
    "            buf_b = buffers[index_b]\n",
    "            no_overlap = buf_a.live_end < buf_b.live_start or buf_b.live_end < buf_a.live_start\n",
    "            if no_overlap:\n",
    "                opportunities.append((buf_a.name, buf_b.name))\n",
    "    return opportunities\n",
    "\n",
    "\n",
    "buffers = [\n",
    "    Buffer(\"input\", 128 * 784 * 4, 0, 1),\n",
    "    Buffer(\"weights\", 784 * 256 * 4, 0, 5),\n",
    "    Buffer(\"matmul_out\", 128 * 256 * 4, 1, 2),\n",
    "    Buffer(\"bias_out\", 128 * 256 * 4, 2, 3),\n",
    "    Buffer(\"relu_out\", 128 * 256 * 4, 3, 5),\n",
    "    Buffer(\"grad_relu\", 128 * 256 * 4, 4, 5),\n",
    "]\n",
    "\n",
    "peak, peak_step, timeline = compute_peak_memory(buffers, total_steps=6)\n",
    "\n",
    "print(\"Memory timeline:\")\n",
    "for step, memory, live_names in timeline:\n",
    "    bar = \"â–ˆ\" * (memory // (128 * 256))\n",
    "    print(f\"  t={step}: {memory / (1024**2):6.2f} MB  [{', '.join(live_names)}] {bar}\")\n",
    "\n",
    "print(f\"\\nPeak memory: {peak / (1024**2):.2f} MB at step {peak_step}\")\n",
    "\n",
    "sharing = find_sharing_opportunities(buffers)\n",
    "print(f\"\\nBuffer sharing opportunities (non-overlapping lifetimes):\")\n",
    "for buf_a, buf_b in sharing:\n",
    "    print(f\"  {buf_a} â†” {buf_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ab1b6",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[ðŸ“˜ OpenXLA Project. *XLA Buffer Assignment.*](https://openxla.org/xla/architecture)\n",
    "\n",
    "[ðŸ“˜ Chen, T. et al. (2016). *Training Deep Nets with Sublinear Memory Cost.* arXiv:1604.06174.](https://arxiv.org/abs/1604.06174)\n",
    "\n",
    "---\n",
    "\n",
    "[â¬…ï¸ Previous: Operator Fusion](./01_operator_fusion.ipynb) | [Next: Dispatch âž¡ï¸](./03_dispatch.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
