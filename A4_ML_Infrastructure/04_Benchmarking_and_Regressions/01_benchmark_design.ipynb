{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c967510b",
   "metadata": {},
   "source": [
    "### A4.4.1. Benchmark Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147d597",
   "metadata": {},
   "source": [
    "> *A well-designed benchmark isolates the code under test, controls for system noise, and produces statistically meaningful timing measurements by choosing appropriate iteration counts, warm-up phases, and metrics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400466e3",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "Benchmarking measures how fast code runs. **Bad benchmarks** produce misleading numbers; **good benchmarks** produce actionable, reproducible measurements.\n",
    "\n",
    "**Benchmark Structure:**\n",
    "\n",
    "1. **Setup** ‚Äî allocate data, initialize state. Not measured.\n",
    "2. **Warm-up** ‚Äî run the code N times to fill caches, trigger JIT compilation, stabilize frequency scaling. Not measured.\n",
    "3. **Measurement loop** ‚Äî run the code M times, record each timing.\n",
    "4. **Reporting** ‚Äî compute statistics (median, mean, std, min, percentiles).\n",
    "\n",
    "**Key Principles:**\n",
    "\n",
    "| Principle | Rationale |\n",
    "|-----------|----------|\n",
    "| Isolate the target | Don't measure setup, teardown, or I/O |\n",
    "| Warm up | JIT, caches, CPU frequency need time to stabilize |\n",
    "| Use median, not mean | Mean is skewed by outliers (GC pauses, OS interrupts) |\n",
    "| Report distribution | Min shows best-case; p99 shows tail latency |\n",
    "| Prevent dead code elimination | Compiler may remove code whose result is unused |\n",
    "| Control input size | Benchmark must cover representative workload sizes |\n",
    "\n",
    "**Common Mistakes:**\n",
    "\n",
    "- Measuring wall time with `time.time()` (low resolution) instead of `time.perf_counter_ns()`.\n",
    "- Too few iterations ‚Üí high variance.\n",
    "- Benchmarking with optimizations disabled.\n",
    "- Comparing benchmarks run on different machines or under different load.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Benchmarking NumPy `dot` on 1000√ó1000 matrices: 5 warm-up iterations, 100 measured iterations, report median and IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd85dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def run_benchmark(target_fn, warmup_iterations, measurement_iterations):\n",
    "    for _ in range(warmup_iterations):\n",
    "        target_fn()\n",
    "\n",
    "    timings_ns = []\n",
    "    for _ in range(measurement_iterations):\n",
    "        start = time.perf_counter_ns()\n",
    "        target_fn()\n",
    "        elapsed = time.perf_counter_ns() - start\n",
    "        timings_ns.append(elapsed)\n",
    "\n",
    "    return np.array(timings_ns)\n",
    "\n",
    "\n",
    "def report_statistics(label, timings_ns):\n",
    "    timings_ms = timings_ns / 1e6\n",
    "    print(f\"{label}:\")\n",
    "    print(f\"  Iterations: {len(timings_ms)}\")\n",
    "    print(f\"  Median: {np.median(timings_ms):.3f} ms\")\n",
    "    print(f\"  Mean:   {np.mean(timings_ms):.3f} ms\")\n",
    "    print(f\"  Std:    {np.std(timings_ms):.3f} ms\")\n",
    "    print(f\"  Min:    {np.min(timings_ms):.3f} ms\")\n",
    "    print(f\"  Max:    {np.max(timings_ms):.3f} ms\")\n",
    "    print(f\"  P25:    {np.percentile(timings_ms, 25):.3f} ms\")\n",
    "    print(f\"  P75:    {np.percentile(timings_ms, 75):.3f} ms\")\n",
    "    print(f\"  P99:    {np.percentile(timings_ms, 99):.3f} ms\")\n",
    "    print(f\"  IQR:    {np.percentile(timings_ms, 75) - np.percentile(timings_ms, 25):.3f} ms\")\n",
    "\n",
    "\n",
    "matrix_size = 512\n",
    "matrix_a = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "matrix_b = np.random.rand(matrix_size, matrix_size).astype(np.float32)\n",
    "\n",
    "timings = run_benchmark(\n",
    "    target_fn=lambda: np.dot(matrix_a, matrix_b),\n",
    "    warmup_iterations=5,\n",
    "    measurement_iterations=50,\n",
    ")\n",
    "\n",
    "report_statistics(f\"np.dot ({matrix_size}x{matrix_size} float32)\", timings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5f5611",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò Gregg, B. (2020). *Systems Performance: Enterprise and the Cloud (2nd ed.).* Addison-Wesley.](https://www.brendangregg.com/systems-performance-2nd-edition-book.html)\n",
    "\n",
    "[üìò Google. *google/benchmark ‚Äî A microbenchmark support library.*](https://github.com/google/benchmark)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: Dispatch](../03_Runtime_Topics/03_dispatch.ipynb) | [Next: Noise Control ‚û°Ô∏è](./02_noise_control.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
