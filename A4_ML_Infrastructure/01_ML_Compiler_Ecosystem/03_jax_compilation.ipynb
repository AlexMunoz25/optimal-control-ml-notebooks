{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f175a31c",
   "metadata": {},
   "source": [
    "### A4.1.3. JAX Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4c7c3",
   "metadata": {},
   "source": [
    "> *JAX traces Python functions into a functional IR (jaxpr), lowers it to StableHLO, and compiles it via XLA ‚Äî making JIT compilation, automatic differentiation, and vectorization composable transformations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b49c1",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "JAX treats numerical Python functions as data: it **traces** them to extract a functional intermediate representation, then **lowers** and **compiles** that IR.\n",
    "\n",
    "**JAX Compilation Pipeline:**\n",
    "\n",
    "```\n",
    "Python function ‚Üí tracing ‚Üí jaxpr ‚Üí StableHLO ‚Üí XLA HLO ‚Üí LLVM/PTX ‚Üí executable\n",
    "```\n",
    "\n",
    "**Tracing:**\n",
    "\n",
    "JAX calls the function with **abstract tracer values** (carrying shape and dtype but no data). Each JAX primitive encountered is recorded into a **jaxpr** (JAX expression) ‚Äî a flat, functional, SSA-like IR.\n",
    "\n",
    "**Key Transformations (composable):**\n",
    "\n",
    "| Transform | Purpose |\n",
    "|-----------|--------|\n",
    "| `jax.jit` | JIT-compile via XLA |\n",
    "| `jax.grad` | Automatic differentiation (reverse-mode) |\n",
    "| `jax.vmap` | Automatic vectorization (batch dimension) |\n",
    "| `jax.pmap` | Parallel map across devices |\n",
    "\n",
    "These compose: `jax.jit(jax.vmap(jax.grad(f)))` is valid.\n",
    "\n",
    "**Caching and Recompilation:**\n",
    "\n",
    "- Compiled executables are cached by **(function identity, input shapes, input dtypes)**.\n",
    "- New shapes trigger recompilation.\n",
    "- `jax.jit` with `static_argnums` treats certain args as compile-time constants.\n",
    "\n",
    "**Lowering Stages:**\n",
    "\n",
    "1. `jax.make_jaxpr(f)(x)` ‚Äî inspect the jaxpr.\n",
    "2. `jax.jit(f).lower(x).as_text()` ‚Äî inspect the StableHLO.\n",
    "3. `jax.jit(f).lower(x).compile()` ‚Äî get the compiled executable.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "@jax.jit\n",
    "def predict(params, x):\n",
    "    return jax.nn.relu(x @ params['w'] + params['b'])\n",
    "```\n",
    "\n",
    "First call traces ‚Üí lowers to StableHLO `dot_general + add + max(0, .)` ‚Üí XLA fuses into one kernel. Subsequent calls with same shapes reuse the cached executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d219b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AbstractValue:\n",
    "    shape: tuple[int, ...]\n",
    "    dtype: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class JaxprEquation:\n",
    "    primitive: str\n",
    "    inputs: list[str]\n",
    "    output: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Jaxpr:\n",
    "    input_vars: list[str]\n",
    "    equations: list[JaxprEquation] = field(default_factory=list)\n",
    "    output_var: str = \"\"\n",
    "\n",
    "\n",
    "jaxpr = Jaxpr(\n",
    "    input_vars=[\"%x\", \"%w\", \"%b\"],\n",
    "    equations=[\n",
    "        JaxprEquation(\"dot_general\", [\"%x\", \"%w\"], \"%mm\"),\n",
    "        JaxprEquation(\"add\", [\"%mm\", \"%b\"], \"%biased\"),\n",
    "        JaxprEquation(\"max\", [\"%biased\", \"%zero\"], \"%out\"),\n",
    "    ],\n",
    "    output_var=\"%out\",\n",
    ")\n",
    "\n",
    "print(\"Jaxpr:\")\n",
    "print(f\"  inputs: {', '.join(jaxpr.input_vars)}\")\n",
    "for equation in jaxpr.equations:\n",
    "    print(f\"  {equation.output} = {equation.primitive}({', '.join(equation.inputs)})\")\n",
    "print(f\"  output: {jaxpr.output_var}\")\n",
    "\n",
    "compilation_stages = [\n",
    "    (\"Python function\", \"def predict(params, x): ...\"),\n",
    "    (\"Tracing\", \"abstract values ‚Üí record primitives\"),\n",
    "    (\"Jaxpr\", \"flat functional SSA IR\"),\n",
    "    (\"StableHLO\", \"dot_general + add + maximum\"),\n",
    "    (\"XLA HLO\", \"fusion, layout, buffer assignment\"),\n",
    "    (\"Machine code\", \"LLVM IR ‚Üí x86 / PTX\"),\n",
    "]\n",
    "\n",
    "print(\"\\nJAX compilation pipeline:\")\n",
    "for stage_name, detail in compilation_stages:\n",
    "    print(f\"  {stage_name} ‚Üí {detail}\")\n",
    "\n",
    "cache_key_a = (\"predict\", ((128, 784), \"f32\"), ((784, 256), \"f32\"), ((256,), \"f32\"))\n",
    "cache_key_b = (\"predict\", ((64, 784), \"f32\"), ((784, 256), \"f32\"), ((256,), \"f32\"))\n",
    "\n",
    "print(f\"\\nCache key (batch=128): {cache_key_a}\")\n",
    "print(f\"Cache key (batch=64):  {cache_key_b}\")\n",
    "print(f\"Same key? {cache_key_a == cache_key_b} ‚Üí recompilation needed for different batch size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4868b8",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò Bradbury, J. et al. *JAX: Composable transformations of Python+NumPy programs.*](https://github.com/google/jax)\n",
    "\n",
    "[üìò JAX Documentation. *How JAX primitives work.*](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_Primitives_Work.html)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: TensorFlow XLA](./02_tensorflow_xla.ipynb) | [Next: TensorFlow Graph Compilation ‚û°Ô∏è](../02_Framework_Integration/01_tensorflow_graph_compilation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
