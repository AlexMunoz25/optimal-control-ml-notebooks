{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d68b10",
   "metadata": {},
   "source": [
    "### A4.1.1. OpenXLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c95f24",
   "metadata": {},
   "source": [
    "> *OpenXLA is an open-source ML compiler ecosystem that provides a shared compiler infrastructure (XLA, StableHLO) across frameworks (TensorFlow, JAX, PyTorch) and hardware targets (CPU, GPU, TPU).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e3a9f",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "The **OpenXLA** project unifies the compilation stack for machine learning so that frameworks do not need per-hardware backends. It centers on two components:\n",
    "\n",
    "**1. StableHLO ‚Äî The Portable IR:**\n",
    "\n",
    "StableHLO (Stable High-Level Operations) is an MLIR dialect that serves as the exchange format between ML frameworks and compilers. It provides:\n",
    "\n",
    "- A **versioned, backward-compatible** operation set.\n",
    "- Operations for tensor compute: `stablehlo.dot_general`, `stablehlo.convolution`, `stablehlo.reduce`.\n",
    "- Framework-agnostic ‚Äî JAX, TensorFlow, and PyTorch/XLA all lower to StableHLO.\n",
    "\n",
    "**2. XLA ‚Äî The Optimizing Compiler:**\n",
    "\n",
    "XLA (Accelerated Linear Algebra) consumes HLO/StableHLO and produces optimized machine code:\n",
    "\n",
    "| Phase | Action |\n",
    "|-------|--------|\n",
    "| HLO optimization | Algebraic simplification, CSE, constant folding |\n",
    "| Fusion | Merge elementwise ops into single kernels |\n",
    "| Layout assignment | Choose memory layout (row-major, tiled) per target |\n",
    "| Code generation | Emit LLVM IR (CPU), PTX (GPU), or HLO‚ÜíTPU instructions |\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "```\n",
    "Framework (JAX / TF / PyTorch)\n",
    "        ‚Üì\n",
    "    StableHLO\n",
    "        ‚Üì\n",
    "    XLA Compiler\n",
    "    ‚îú‚îÄ‚îÄ CPU codegen (LLVM)\n",
    "    ‚îú‚îÄ‚îÄ GPU codegen (LLVM ‚Üí PTX)\n",
    "    ‚îî‚îÄ‚îÄ TPU codegen\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "\n",
    "A JAX function `jnp.dot(a, b) + c` lowers to StableHLO ops `stablehlo.dot_general` + `stablehlo.add`, which XLA fuses into a single kernel with an optimized GEMM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d91e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StableHLOOp:\n",
    "    name: str\n",
    "    operands: list[str]\n",
    "    result: str\n",
    "    result_shape: tuple[int, ...]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HLOModule:\n",
    "    name: str\n",
    "    ops: list[StableHLOOp] = field(default_factory=list)\n",
    "\n",
    "    def add_op(self, op_name, operands, result, result_shape):\n",
    "        operation = StableHLOOp(op_name, operands, result, result_shape)\n",
    "        self.ops.append(operation)\n",
    "        return operation\n",
    "\n",
    "\n",
    "module = HLOModule(\"matmul_add\")\n",
    "module.add_op(\"stablehlo.dot_general\", [\"%a\", \"%b\"], \"%dot\", (128, 64))\n",
    "module.add_op(\"stablehlo.add\", [\"%dot\", \"%bias\"], \"%result\", (128, 64))\n",
    "\n",
    "print(f\"Module: {module.name}\")\n",
    "print(f\"Operations: {len(module.ops)}\")\n",
    "for op in module.ops:\n",
    "    print(f\"  {op.result} = {op.name}({', '.join(op.operands)}) : tensor<{'x'.join(str(d) for d in op.result_shape)}xf32>\")\n",
    "\n",
    "xla_phases = [\n",
    "    (\"HLO optimization\", \"algebraic simplification, CSE, constant folding\"),\n",
    "    (\"Fusion\", \"merge dot_general + add into fused kernel\"),\n",
    "    (\"Layout assignment\", \"choose row-major for CPU, tiled for GPU\"),\n",
    "    (\"Code generation\", \"emit LLVM IR or PTX\"),\n",
    "]\n",
    "\n",
    "print(\"\\nXLA compilation pipeline:\")\n",
    "for phase_name, description in xla_phases:\n",
    "    print(f\"  {phase_name}: {description}\")\n",
    "\n",
    "targets = {\"CPU\": \"LLVM IR ‚Üí x86/ARM\", \"GPU\": \"LLVM IR ‚Üí PTX ‚Üí SASS\", \"TPU\": \"HLO ‚Üí TPU instructions\"}\n",
    "print(\"\\nBackend targets:\")\n",
    "for target, codegen in targets.items():\n",
    "    print(f\"  {target}: {codegen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368779c7",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò OpenXLA Project. *OpenXLA ‚Äî An Open Ecosystem of ML Compilers.*](https://openxla.org/)\n",
    "\n",
    "[üìò OpenXLA Project. *StableHLO Specification.*](https://openxla.org/stablehlo)\n",
    "\n",
    "---\n",
    "\n",
    "[Next: TensorFlow XLA ‚û°Ô∏è](./02_tensorflow_xla.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
