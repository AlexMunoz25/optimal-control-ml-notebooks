{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd992e0",
   "metadata": {},
   "source": [
    "### A4.2.3. Custom Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b315b",
   "metadata": {},
   "source": [
    "> *A custom call is an XLA mechanism that invokes an external function (hand-written kernel, library routine) from within a compiled HLO graph, bridging the gap between compiler-generated and manually-optimized code.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cf635",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "Not every operation can be expressed efficiently as a composition of XLA primitives. **Custom calls** allow the compiled graph to call out to:\n",
    "\n",
    "- **Vendor libraries** ‚Äî cuBLAS GEMM, cuDNN convolution, MKL routines.\n",
    "- **Hand-tuned kernels** ‚Äî Triton kernels, inline PTX, hand-written CUDA.\n",
    "- **External C/C++ functions** ‚Äî FFI (Foreign Function Interface) targets.\n",
    "\n",
    "**In XLA HLO:**\n",
    "\n",
    "```\n",
    "%result = custom-call(%input), custom_call_target=\"my_kernel\",\n",
    "    backend_config={...}, api_version=API_VERSION_TYPED_FFI\n",
    "```\n",
    "\n",
    "**In JAX:**\n",
    "\n",
    "JAX exposes custom calls via `jax.extend.ffi.ffi_call` (the new FFI API) or the older `jax.lib.xla_client.register_custom_call_target`.\n",
    "\n",
    "Requirements for a custom call:\n",
    "\n",
    "1. **Registration** ‚Äî register the function name and pointer with XLA.\n",
    "2. **Shape inference** ‚Äî tell the compiler the output shape given input shapes.\n",
    "3. **Differentiation rule** ‚Äî if the op must participate in `jax.grad`, provide a custom VJP.\n",
    "4. **Batching rule** ‚Äî if it must work with `jax.vmap`, provide a batching rule.\n",
    "\n",
    "**Use Cases:**\n",
    "\n",
    "| Scenario | Why custom call? |\n",
    "|----------|------------------|\n",
    "| cuBLAS GEMM | XLA's generated GEMM may be slower than cuBLAS for certain shapes |\n",
    "| Sparse ops | XLA has no native sparse support |\n",
    "| Hardware-specific intrinsics | TPU intrinsics, GPU tensor cores |\n",
    "| Third-party libraries | NCCL collectives, CUTLASS |\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from jax.extend import ffi\n",
    "\n",
    "def my_custom_op(x):\n",
    "    return ffi.ffi_call(\"my_kernel\", result_shape=x)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b764cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomCallTarget:\n",
    "    name: str\n",
    "    platform: str\n",
    "    has_grad_rule: bool\n",
    "    has_batch_rule: bool\n",
    "\n",
    "\n",
    "def infer_output_shape(target_name, input_shapes):\n",
    "    shape_rules = {\n",
    "        \"cublas_gemm\": lambda shapes: (shapes[0][0], shapes[1][1]),\n",
    "        \"custom_relu\": lambda shapes: shapes[0],\n",
    "        \"sparse_matmul\": lambda shapes: (shapes[0][0], shapes[1][1]),\n",
    "    }\n",
    "    return shape_rules[target_name](input_shapes)\n",
    "\n",
    "\n",
    "registered_targets = [\n",
    "    CustomCallTarget(\"cublas_gemm\", \"gpu\", has_grad_rule=True, has_batch_rule=True),\n",
    "    CustomCallTarget(\"custom_relu\", \"gpu\", has_grad_rule=True, has_batch_rule=True),\n",
    "    CustomCallTarget(\"sparse_matmul\", \"cpu\", has_grad_rule=False, has_batch_rule=False),\n",
    "    CustomCallTarget(\"nccl_allreduce\", \"gpu\", has_grad_rule=True, has_batch_rule=False),\n",
    "]\n",
    "\n",
    "print(\"Registered custom call targets:\")\n",
    "for target in registered_targets:\n",
    "    grad = \"‚úì\" if target.has_grad_rule else \"‚úó\"\n",
    "    batch = \"‚úì\" if target.has_batch_rule else \"‚úó\"\n",
    "    print(f\"  {target.name} ({target.platform}) ‚Äî grad: {grad}, vmap: {batch}\")\n",
    "\n",
    "gemm_output = infer_output_shape(\"cublas_gemm\", [(128, 784), (784, 256)])\n",
    "relu_output = infer_output_shape(\"custom_relu\", [(128, 256)])\n",
    "\n",
    "print(f\"\\nShape inference:\")\n",
    "print(f\"  cublas_gemm((128,784), (784,256)) ‚Üí {gemm_output}\")\n",
    "print(f\"  custom_relu((128,256)) ‚Üí {relu_output}\")\n",
    "\n",
    "hlo_representation = [\n",
    "    '%p0 = f32[128,784] parameter(0)',\n",
    "    '%p1 = f32[784,256] parameter(1)',\n",
    "    '%gemm = f32[128,256] custom-call(%p0, %p1), custom_call_target=\"cublas_gemm\"',\n",
    "    '%out = f32[128,256] custom-call(%gemm), custom_call_target=\"custom_relu\"',\n",
    "]\n",
    "\n",
    "print(f\"\\nHLO with custom calls:\")\n",
    "for line in hlo_representation:\n",
    "    print(f\"  {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c190649",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[üìò JAX Documentation. *FFI ‚Äî Foreign Function Interface.*](https://jax.readthedocs.io/en/latest/ffi.html)\n",
    "\n",
    "[üìò XLA Documentation. *Custom Calls.*](https://openxla.org/xla/custom_call)\n",
    "\n",
    "---\n",
    "\n",
    "[‚¨ÖÔ∏è Previous: JAX Just-in-Time Compilation](./02_jax_just_in_time_compilation.ipynb) | [Next: Operator Fusion ‚û°Ô∏è](../03_Runtime_Topics/01_operator_fusion.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
